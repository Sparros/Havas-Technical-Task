{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ee9368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAIN_CSV : data\\rainfall_nightbefore.csv\n",
      "RAIL_CSV : data\\rail_metrics_LDS_KGX_2022-05-01_to_2022-05-30.csv\n",
      "FINAL_CSV: data\\final_LDS_KGX_2022-05-01_to_2022-05-30.csv\n",
      "CEDA_ACCESS_TOKEN set: True\n",
      "RAIL_USER set: True\n",
      "RAIL_PASSWORD set: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ---- Date range for the task ----\n",
    "START_DATE = \"2022-05-01\"\n",
    "END_DATE   = \"2022-05-30\"\n",
    "\n",
    "# ---- Route ----\n",
    "FROM_LOC = \"LDS\"\n",
    "TO_LOC   = \"KGX\"\n",
    "\n",
    "# ---- CEDA URL ----\n",
    "CEDA_URL = \"https://dap.ceda.ac.uk/badc/ukmo-midas-open/data/uk-hourly-rain-obs/dataset-version-202507/west-yorkshire/00534_bramham/qc-version-1/midas-open_uk-hourly-rain-obs_dv-202507_west-yorkshire_00534_bramham_qcv-1_2022.csv?download=1\"\n",
    "\n",
    "# ---- Output filenames ----\n",
    "RAIN_CSV  = DATA_DIR / \"rainfall_nightbefore.csv\"\n",
    "RAIL_CSV  = DATA_DIR / f\"rail_metrics_{FROM_LOC}_{TO_LOC}_{START_DATE}_to_{END_DATE}.csv\"\n",
    "FINAL_CSV = DATA_DIR / f\"final_{FROM_LOC}_{TO_LOC}_{START_DATE}_to_{END_DATE}.csv\"\n",
    "\n",
    "print(\"RAIN_CSV :\", RAIN_CSV)\n",
    "print(\"RAIL_CSV :\", RAIL_CSV)\n",
    "print(\"FINAL_CSV:\", FINAL_CSV)\n",
    "\n",
    "print(\"CEDA_ACCESS_TOKEN set:\", bool(os.getenv(\"CEDA_ACCESS_TOKEN\")))\n",
    "print(\"RAIL_USER set:\", bool(os.getenv(\"RAIL_USER\")))\n",
    "print(\"RAIL_PASSWORD set:\", bool(os.getenv(\"RAIL_PASSWORD\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18371ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GCS_CACHE = True\n",
    "BUCKET_NAME = \"de-candidate-task-results-sp\"\n",
    "SERVICE_ACCOUNT_FILE = \"searchlab-bq-training-sp-key.json\"  # <-- your JSON key file\n",
    "\n",
    "def ensure_local_from_gcs(bucket_name, blob_name, local_path, service_account_file):\n",
    "    \"\"\"\n",
    "    If local_path doesn't exist and blob exists in GCS, download it.\n",
    "    Returns True if local file exists after this function.\n",
    "    \"\"\"\n",
    "    if local_path.exists():\n",
    "        return True\n",
    "\n",
    "    if not USE_GCS_CACHE:\n",
    "        return False\n",
    "\n",
    "    from google.cloud import storage\n",
    "    from google.oauth2 import service_account\n",
    "\n",
    "    creds = service_account.Credentials.from_service_account_file(service_account_file)\n",
    "    client = storage.Client(credentials=creds)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    if blob.exists(client):\n",
    "        local_path.parent.mkdir(exist_ok=True)\n",
    "        blob.download_to_filename(str(local_path))\n",
    "        print(f\"[INFO] Downloaded cache gs://{bucket_name}/{blob_name} -> {local_path}\")\n",
    "        return True\n",
    "\n",
    "    print(f\"[INFO] No existing blob found: gs://{bucket_name}/{blob_name}\")\n",
    "    return False\n",
    "\n",
    "\n",
    "def upload_file_to_gcs(local_file_path, bucket_name, blob_name, service_account_file):\n",
    "    from google.cloud import storage\n",
    "    from google.oauth2 import service_account\n",
    "\n",
    "    creds = service_account.Credentials.from_service_account_file(service_account_file)\n",
    "    client = storage.Client(credentials=creds)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    blob.upload_from_filename(str(local_file_path))\n",
    "    print(f\"[INFO] Uploaded -> gs://{bucket_name}/{blob_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc8e508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded rainfall cache -> data\\rainfall_nightbefore.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>weather_station</th>\n",
       "      <th>rainfall_nightbefore_mm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>bramham</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>bramham</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-04</td>\n",
       "      <td>bramham</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-05</td>\n",
       "      <td>bramham</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-06</td>\n",
       "      <td>bramham</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date weather_station  rainfall_nightbefore_mm\n",
       "0  2022-01-02         bramham                      0.0\n",
       "1  2022-01-03         bramham                      1.6\n",
       "2  2022-01-04         bramham                      1.0\n",
       "3  2022-01-05         bramham                      0.0\n",
       "4  2022-01-06         bramham                      0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "RAIN_GCS_BLOB = RAIN_CSV.name  # rainfall_nightbefore.csv in the bucket\n",
    "\n",
    "def compute_night_before_rainfall(weather_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = weather_df.copy()\n",
    "    df[\"ob_end_time\"] = pd.to_datetime(df[\"ob_end_time\"], errors=\"coerce\")\n",
    "    df[\"prcp_amt\"] = pd.to_numeric(df[\"prcp_amt\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # Filter 18:00â€“23:00\n",
    "    df = df[df[\"ob_end_time\"].dt.hour.between(18, 23)]\n",
    "\n",
    "    agg = (\n",
    "        df.groupby([df[\"ob_end_time\"].dt.date, \"weather_station\"])[\"prcp_amt\"]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"ob_end_time\": \"obs_date\", \"prcp_amt\": \"rainfall_evening_mm\"})\n",
    "    )\n",
    "\n",
    "    # Shift +1 day to become \"night before\"\n",
    "    agg[\"date\"] = pd.to_datetime(agg[\"obs_date\"]) + pd.Timedelta(days=1)\n",
    "    agg[\"date\"] = agg[\"date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    return agg[[\"date\", \"weather_station\", \"rainfall_evening_mm\"]].rename(\n",
    "        columns={\"rainfall_evening_mm\": \"rainfall_nightbefore_mm\"}\n",
    "    )\n",
    "\n",
    "# Prefer local rainfall cache; else download from GCS; else compute\n",
    "have_rain = ensure_local_from_gcs(BUCKET_NAME, RAIN_GCS_BLOB, RAIN_CSV, SERVICE_ACCOUNT_FILE)\n",
    "\n",
    "if have_rain:\n",
    "    rainfall_df = pd.read_csv(RAIN_CSV)\n",
    "    print(f\"[INFO] Loaded rainfall cache -> {RAIN_CSV}\")\n",
    "else:\n",
    "    from ceda_utils import get_weather_data\n",
    "\n",
    "    CEDA_ACCESS_TOKEN = os.getenv(\"CEDA_ACCESS_TOKEN\")\n",
    "    if not CEDA_ACCESS_TOKEN:\n",
    "        raise ValueError(\"CEDA_ACCESS_TOKEN not set\")\n",
    "\n",
    "    weather_df = get_weather_data(url=CEDA_URL, access_token=CEDA_ACCESS_TOKEN)\n",
    "    rainfall_df = compute_night_before_rainfall(weather_df)\n",
    "\n",
    "    rainfall_df.to_csv(RAIN_CSV, index=False)\n",
    "    print(f\"[INFO] Saved rainfall CSV -> {RAIN_CSV}\")\n",
    "\n",
    "    if USE_GCS_CACHE:\n",
    "        upload_file_to_gcs(RAIN_CSV, BUCKET_NAME, RAIN_GCS_BLOB, SERVICE_ACCOUNT_FILE)\n",
    "\n",
    "rainfall_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21976d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Cached days: 0\n",
      "[INFO] Missing days: 30\n",
      "[INFO] Missing sample: ['2022-05-01', '2022-05-02', '2022-05-03', '2022-05-04', '2022-05-05', '2022-05-06', '2022-05-07', '2022-05-08', '2022-05-09', '2022-05-10']\n",
      "[WARNING] Status 502: <!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\n",
      "<html><head>\n",
      "<title>502 Proxy Error</title>\n",
      "</head><body>\n",
      "<h1>Proxy E\n",
      "[WARNING] Status 502: <!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\n",
      "<html><head>\n",
      "<title>502 Proxy Error</title>\n",
      "</head><body>\n",
      "<h1>Proxy E\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from base64 import b64encode\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "HSP_URL = \"https://hsp-prod.rockshore.net/api/v1/serviceMetrics\"\n",
    "RAIL_GCS_BLOB = RAIL_CSV.name\n",
    "\n",
    "# Download rail cache only if local missing\n",
    "ensure_local_from_gcs(BUCKET_NAME, RAIL_GCS_BLOB, RAIL_CSV, SERVICE_ACCOUNT_FILE)\n",
    "\n",
    "def call_hsp_api(url, headers, payload, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                url,\n",
    "                headers=headers,\n",
    "                json=payload,\n",
    "                timeout=90\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "\n",
    "            print(f\"[WARNING] Status {response.status_code}: {response.text[:200]}\")\n",
    "\n",
    "        except requests.exceptions.ReadTimeout:\n",
    "            print(f\"[WARNING] Timeout attempt {attempt + 1}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Attempt {attempt + 1}: {e}\")\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "    print(\"[ERROR] All retries failed\")\n",
    "    return None\n",
    "\n",
    "def get_daily_service_metrics(from_loc, to_loc, start_date, end_date):\n",
    "    username = os.getenv(\"RAIL_USER\")\n",
    "    password = os.getenv(\"RAIL_PASSWORD\")\n",
    "    if not username or not password:\n",
    "        raise ValueError(\"RAIL_USER and RAIL_PASSWORD must be set\")\n",
    "\n",
    "    token = b64encode(f\"{username}:{password}\".encode()).decode()\n",
    "    headers = {\"Authorization\": f\"Basic {token}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    current_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_date_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "    records = []\n",
    "    while current_date <= end_date_dt:\n",
    "        date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        payload = {\n",
    "            \"from_loc\": from_loc,\n",
    "            \"to_loc\": to_loc,\n",
    "            \"from_time\": \"0600\",\n",
    "            \"to_time\": \"0959\",\n",
    "            \"from_date\": date_str,\n",
    "            \"to_date\": date_str,\n",
    "            \"days\": \"WEEKDAY\",\n",
    "            \"tolerance\": [\"5\"],\n",
    "        }\n",
    "\n",
    "        print(f\"[INFO] Fetching rail data for {date_str}\")\n",
    "        data = call_hsp_api(HSP_URL, headers, payload)\n",
    "\n",
    "        if not data:\n",
    "            print(f\"[ERROR] Skipping {date_str}\")\n",
    "            current_date += timedelta(days=1)\n",
    "            continue\n",
    "\n",
    "        total = 0\n",
    "        on_time = 0\n",
    "        for svc in data.get(\"Services\", []):\n",
    "            metrics = svc.get(\"Metrics\", [])\n",
    "            if not metrics:\n",
    "                continue\n",
    "            m = metrics[0]\n",
    "            total += int(m[\"num_not_tolerance\"]) + int(m[\"num_tolerance\"])\n",
    "            on_time += int(m[\"num_tolerance\"])\n",
    "\n",
    "        records.append({\n",
    "            \"date\": date_str,\n",
    "            \"departure_rail_station_crs\": from_loc,\n",
    "            \"destination_rail_station_crs\": to_loc,\n",
    "            \"service_count_total\": total,\n",
    "            \"service_count_ontime\": on_time,\n",
    "        })\n",
    "\n",
    "        time.sleep(1)  # polite delay\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def daterange(start_date: str, end_date: str):\n",
    "    cur = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    while cur <= end:\n",
    "        yield cur.strftime(\"%Y-%m-%d\")\n",
    "        cur += timedelta(days=1)\n",
    "\n",
    "def load_cached_csv(path: Path):\n",
    "    if path.exists():\n",
    "        return pd.read_csv(path)\n",
    "    return None\n",
    "\n",
    "def cached_dates(df: pd.DataFrame | None):\n",
    "    if df is None or df.empty or \"date\" not in df.columns:\n",
    "        return set()\n",
    "    return set(df[\"date\"].astype(str))\n",
    "\n",
    "def upsert(existing_df: pd.DataFrame | None, new_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if existing_df is None or existing_df.empty:\n",
    "        return new_df.copy()\n",
    "    combined = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "    combined = combined.drop_duplicates(\n",
    "        subset=[\"date\",\"departure_rail_station_crs\",\"destination_rail_station_crs\"],\n",
    "        keep=\"last\"\n",
    "    )\n",
    "    return combined.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# ---- caching logic ----\n",
    "existing_rail_df = load_cached_csv(RAIL_CSV)\n",
    "done = cached_dates(existing_rail_df)\n",
    "\n",
    "all_dates = set(daterange(START_DATE, END_DATE))\n",
    "missing = sorted(all_dates - done)\n",
    "\n",
    "print(f\"[INFO] Cached days: {len(done)}\")\n",
    "print(f\"[INFO] Missing days: {len(missing)}\")\n",
    "print(\"[INFO] Missing sample:\", missing[:10])\n",
    "\n",
    "if missing:\n",
    "    # fetch smallest range covering missing dates, then filter down\n",
    "    fetch_start = min(missing)\n",
    "    fetch_end   = max(missing)\n",
    "\n",
    "    fetched_df = get_daily_service_metrics(FROM_LOC, TO_LOC, fetch_start, fetch_end)\n",
    "    fetched_df = fetched_df[fetched_df[\"date\"].isin(missing)].reset_index(drop=True)\n",
    "\n",
    "    rail_df = upsert(existing_rail_df, fetched_df)\n",
    "else:\n",
    "    rail_df = existing_rail_df.copy() if existing_rail_df is not None else pd.DataFrame()\n",
    "\n",
    "rail_df.to_csv(RAIL_CSV, index=False)\n",
    "print(f\"[INFO] Saved rail CSV -> {RAIL_CSV}\")\n",
    "\n",
    "# Upload only if we actually fetched new data\n",
    "if USE_GCS_CACHE and missing:\n",
    "    upload_file_to_gcs(RAIL_CSV, BUCKET_NAME, RAIL_GCS_BLOB, SERVICE_ACCOUNT_FILE)\n",
    "\n",
    "rail_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c038a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_final_dataset(rail_df: pd.DataFrame, rainfall_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = rail_df.merge(rainfall_df, on=\"date\", how=\"left\")\n",
    "\n",
    "    out[\"rainfall_nightbefore_mm\"] = pd.to_numeric(out[\"rainfall_nightbefore_mm\"], errors=\"coerce\").fillna(0.0)\n",
    "    out[\"service_count_total\"] = pd.to_numeric(out[\"service_count_total\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    out[\"service_count_ontime\"] = pd.to_numeric(out[\"service_count_ontime\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    out = out[\n",
    "        [\n",
    "            \"date\",\n",
    "            \"departure_rail_station_crs\",\n",
    "            \"destination_rail_station_crs\",\n",
    "            \"weather_station\",\n",
    "            \"rainfall_nightbefore_mm\",\n",
    "            \"service_count_total\",\n",
    "            \"service_count_ontime\",\n",
    "        ]\n",
    "    ]\n",
    "    return out\n",
    "\n",
    "final_df = build_final_dataset(rail_df, rainfall_df)\n",
    "final_df.to_csv(FINAL_CSV, index=False)\n",
    "print(f\"[INFO] Saved final CSV -> {FINAL_CSV}\")\n",
    "\n",
    "if USE_GCS_CACHE:\n",
    "    upload_file_to_gcs(FINAL_CSV, BUCKET_NAME, FINAL_CSV.name, SERVICE_ACCOUNT_FILE)\n",
    "\n",
    "final_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c43ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates = set(daterange(START_DATE, END_DATE))\n",
    "have_dates = set(rail_df[\"date\"].astype(str)) if not rail_df.empty else set()\n",
    "still_missing = sorted(all_dates - have_dates)\n",
    "\n",
    "print(\"[INFO] Still missing rail dates:\", len(still_missing))\n",
    "print(still_missing[:20])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
