{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54ee9368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured routes: 2\n",
      "LDS -> KGX | weather: bramham 00534\n",
      "LDS -> BDQ | weather: bradford 00516\n",
      "CEDA_ACCESS_TOKEN set: True\n",
      "RAIL_USER set: True\n",
      "RAIL_PASSWORD set: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Local data cache directory. Every run can re-use prior outputs and only fetch missing data.\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Task date window (drives both rainfall aggregation and rail query)\n",
    "START_DATE = \"2022-05-01\"\n",
    "END_DATE   = \"2022-05-30\"\n",
    "\n",
    "# Turn on/off cloud caching. When enabled:\n",
    "# - if local CSV missing, attempt to download it from GCS\n",
    "# - after computing outputs, upload them to GCS for future runs / sharing.\n",
    "USE_GCS_CACHE = True\n",
    "BUCKET_NAME = \"de-candidate-task-results-sp\"\n",
    "SERVICE_ACCOUNT_FILE = \"searchlab-bq-training-sp-key.json\"\n",
    "\n",
    "# Business rule for the \"night before\" rainfall feature:\n",
    "# Sum precipitation between 18:00–23:00 on day D-1 and attach it to rail date D.\n",
    "RAINFALL_START_TIME = 18\n",
    "RAINFALL_END_TIME   = 23   \n",
    "\n",
    "# Each route maps to a rail origin/destination AND a weather station (CEDA MIDAS).\n",
    "# This keeps the pipeline scalable: add routes by adding dicts, no code changes.\n",
    "ROUTES = [\n",
    "    {\n",
    "        \"from_loc\": \"LDS\",\n",
    "        \"to_loc\": \"KGX\",\n",
    "        \"weather_station_name\": \"bramham\",\n",
    "        \"weather_region\": \"west-yorkshire\",\n",
    "        \"weather_station_id\": \"00534\",\n",
    "        \"ceda_csv_url\": \"https://dap.ceda.ac.uk/badc/ukmo-midas-open/data/uk-hourly-rain-obs/dataset-version-202507/west-yorkshire/00534_bramham/qc-version-1/midas-open_uk-hourly-rain-obs_dv-202507_west-yorkshire_00534_bramham_qcv-1_2022.csv?download=1\",\n",
    "    },\n",
    "    {\n",
    "        \"from_loc\": \"LDS\",\n",
    "        \"to_loc\": \"BDQ\",\n",
    "        \"weather_station_name\": \"bradford\",\n",
    "        \"weather_region\": \"west-yorkshire\",\n",
    "        \"weather_station_id\": \"00516\",\n",
    "        \"ceda_csv_url\": \"https://dap.ceda.ac.uk/badc/ukmo-midas-open/data/uk-hourly-rain-obs/dataset-version-202507/west-yorkshire/00516_bradford/qc-version-1/midas-open_uk-hourly-rain-obs_dv-202507_west-yorkshire_00516_bradford_qcv-1_2022.csv?download=1\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Basic run visibility: helps during development and shows config is loaded correctly.\n",
    "print(\"Configured routes:\", len(ROUTES))\n",
    "for r in ROUTES:\n",
    "    print(r[\"from_loc\"], \"->\", r[\"to_loc\"], \"| weather:\", r[\"weather_station_name\"], r[\"weather_station_id\"])\n",
    "\n",
    "# Environment variables keep secrets out of source code and support CI / deployment later.\n",
    "print(\"CEDA_ACCESS_TOKEN set:\", bool(os.getenv(\"CEDA_ACCESS_TOKEN\")))\n",
    "print(\"RAIL_USER set:\", bool(os.getenv(\"RAIL_USER\")))\n",
    "print(\"RAIL_PASSWORD set:\", bool(os.getenv(\"RAIL_PASSWORD\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d18371ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_local_from_gcs(bucket_name, blob_name, local_path, service_account_file):\n",
    "    \"\"\"\n",
    "    Cache strategy:\n",
    "      - If local file exists, use it (fastest).\n",
    "      - Else, if cloud caching enabled and blob exists, download from GCS.\n",
    "      - Else, caller must compute/fetch it.\n",
    "    \"\"\"\n",
    "    if local_path.exists():\n",
    "        return True\n",
    "    if not USE_GCS_CACHE:\n",
    "        return False\n",
    "\n",
    "    from google.cloud import storage\n",
    "    from google.oauth2 import service_account\n",
    "\n",
    "    creds = service_account.Credentials.from_service_account_file(service_account_file)\n",
    "    client = storage.Client(credentials=creds)\n",
    "\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    if blob.exists(client):\n",
    "        local_path.parent.mkdir(exist_ok=True)\n",
    "        blob.download_to_filename(str(local_path))\n",
    "        print(f\"[INFO] Downloaded gs://{bucket_name}/{blob_name} -> {local_path}\")\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def upload_file_to_gcs(local_file_path, bucket_name, blob_name, service_account_file):\n",
    "    \"\"\"\n",
    "    Upload computed outputs to GCS so future runs can avoid re-fetching/recomputing.\n",
    "    This also makes results easy to share and easy to load into BigQuery from GCS.\n",
    "    \"\"\"\n",
    "    if not USE_GCS_CACHE:\n",
    "        return\n",
    "\n",
    "    from google.cloud import storage\n",
    "    from google.oauth2 import service_account\n",
    "\n",
    "    creds = service_account.Credentials.from_service_account_file(service_account_file)\n",
    "    client = storage.Client(credentials=creds)\n",
    "\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.upload_from_filename(str(local_file_path))\n",
    "    print(f\"[INFO] Uploaded -> gs://{bucket_name}/{blob_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fc8e508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ceda_utils import get_weather_data\n",
    "\n",
    "def compute_night_before_rainfall(weather_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts raw hourly rainfall observations into a daily \"night before\" feature.\n",
    "\n",
    "    Logic:\n",
    "      - parse timestamps\n",
    "      - coerce precipitation to numeric\n",
    "      - filter to evening hours (18:00–23:00)\n",
    "      - sum by day and station\n",
    "      - shift forward by +1 day so it aligns with the next day's rail date\n",
    "\n",
    "    Output schema:\n",
    "      date (YYYY-MM-DD), weather_station, rainfall_nightbefore_mm\n",
    "    \"\"\"\n",
    "    df = weather_df.copy()\n",
    "\n",
    "    # Robust parsing: handle unexpected values gracefully.\n",
    "    df[\"ob_end_time\"] = pd.to_datetime(df[\"ob_end_time\"], errors=\"coerce\")\n",
    "    df[\"prcp_amt\"] = pd.to_numeric(df[\"prcp_amt\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # Feature window: evening prior to rail date.\n",
    "    df = df[df[\"ob_end_time\"].dt.hour.between(RAINFALL_START_TIME, RAINFALL_END_TIME)]\n",
    "\n",
    "    # Aggregate rainfall by calendar date and station\n",
    "    agg = (\n",
    "        df.groupby([df[\"ob_end_time\"].dt.date, \"weather_station\"])[\"prcp_amt\"]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"ob_end_time\": \"obs_date\", \"prcp_amt\": \"rainfall_evening_mm\"})\n",
    "    )\n",
    "\n",
    "    # Shift date forward: evening of D-1 becomes rainfall feature for rail date D\n",
    "    agg[\"date\"] = pd.to_datetime(agg[\"obs_date\"]) + pd.Timedelta(days=1)\n",
    "    agg[\"date\"] = agg[\"date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    return agg[[\"date\", \"weather_station\", \"rainfall_evening_mm\"]].rename(\n",
    "        columns={\"rainfall_evening_mm\": \"rainfall_nightbefore_mm\"}\n",
    "    )\n",
    "\n",
    "\n",
    "def get_or_build_rainfall(route: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rainfall caching strategy:\n",
    "      - Use local cached rainfall if present\n",
    "      - Else download from GCS cache (if enabled)\n",
    "      - Else fetch from CEDA and compute the feature, then cache locally and to GCS\n",
    "\n",
    "    Rationale:\n",
    "      - Weather data is stable for historical periods → caching is safe and saves time.\n",
    "      - One rainfall CSV per station/route prevents mixing stations with different rainfall patterns.\n",
    "    \"\"\"\n",
    "    station_id = route[\"weather_station_id\"]\n",
    "    station_name = route[\"weather_station_name\"]\n",
    "\n",
    "    rain_csv = DATA_DIR / f\"rainfall_{station_id}_{station_name}_{START_DATE}_to_{END_DATE}.csv\"\n",
    "    rain_blob = rain_csv.name\n",
    "\n",
    "    have = ensure_local_from_gcs(BUCKET_NAME, rain_blob, rain_csv, SERVICE_ACCOUNT_FILE)\n",
    "    if have:\n",
    "        df = pd.read_csv(rain_csv)\n",
    "        print(f\"[INFO] Loaded rainfall cache -> {rain_csv}\")\n",
    "        return df\n",
    "\n",
    "    token = os.getenv(\"CEDA_ACCESS_TOKEN\")\n",
    "    if not token:\n",
    "        raise ValueError(\"CEDA_ACCESS_TOKEN not set\")\n",
    "\n",
    "    weather_df = get_weather_data(url=route[\"ceda_csv_url\"], access_token=token)\n",
    "    df = compute_night_before_rainfall(weather_df)\n",
    "\n",
    "    df.to_csv(rain_csv, index=False)\n",
    "    print(f\"[INFO] Saved rainfall -> {rain_csv}\")\n",
    "\n",
    "    upload_file_to_gcs(rain_csv, BUCKET_NAME, rain_blob, SERVICE_ACCOUNT_FILE)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f259d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from base64 import b64encode\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "\n",
    "HSP_URL = \"https://hsp-prod.rockshore.net/api/v1/serviceMetrics\"\n",
    "\n",
    "def call_hsp_api(url, headers, payload, retries=3):\n",
    "    \"\"\"\n",
    "    HSP API is known to intermittently return 5xx and time out.\n",
    "    This wrapper provides simple resilience:\n",
    "      - retry on timeouts / errors\n",
    "      - fixed backoff between retries\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=payload, timeout=90)\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            print(f\"[WARNING] Status {response.status_code}: {response.text[:200]}\")\n",
    "        except requests.exceptions.ReadTimeout:\n",
    "            print(f\"[WARNING] Timeout attempt {attempt + 1}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Attempt {attempt + 1}: {e}\")\n",
    "\n",
    "        time.sleep(3)  # backoff to avoid hammering an unstable endpoint\n",
    "\n",
    "    print(\"[ERROR] All retries failed\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_daily_service_metrics(from_loc, to_loc, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Calls serviceMetrics once per day and aggregates totals for the date.\n",
    "\n",
    "    Output schema matches task requirement:\n",
    "      date, departure_rail_station_crs, destination_rail_station_crs,\n",
    "      service_count_total, service_count_ontime\n",
    "\n",
    "    Note:\n",
    "      - We use tolerance=[\"5\"] so \"on time\" means within 5 minutes.\n",
    "      - We query a morning time window (06:00–09:59) as part of task assumptions.\n",
    "    \"\"\"\n",
    "    username = os.getenv(\"RAIL_USER\")\n",
    "    password = os.getenv(\"RAIL_PASSWORD\")\n",
    "    if not username or not password:\n",
    "        raise ValueError(\"RAIL_USER and RAIL_PASSWORD must be set\")\n",
    "\n",
    "    # Basic Auth header required by HSP API (username:password base64)\n",
    "    token = b64encode(f\"{username}:{password}\".encode()).decode()\n",
    "    headers = {\"Authorization\": f\"Basic {token}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    cur = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "    records = []\n",
    "\n",
    "    while cur <= end:\n",
    "        date_str = cur.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        payload = {\n",
    "            \"from_loc\": from_loc,\n",
    "            \"to_loc\": to_loc,\n",
    "            \"from_time\": \"0600\",\n",
    "            \"to_time\": \"0959\",\n",
    "            \"from_date\": date_str,\n",
    "            \"to_date\": date_str,\n",
    "            \"days\": \"WEEKDAY\",\n",
    "            \"tolerance\": [\"5\"],\n",
    "        }\n",
    "\n",
    "        print(f\"[INFO] Fetching rail {from_loc}->{to_loc} {date_str}\")\n",
    "        data = call_hsp_api(HSP_URL, headers, payload)\n",
    "\n",
    "        if not data:\n",
    "            # We skip failed days rather than failing the entire pipeline.\n",
    "            # This supports \"rerun to fill gaps\".\n",
    "            print(f\"[ERROR] Skipping {date_str}\")\n",
    "            cur += timedelta(days=1)\n",
    "            continue\n",
    "\n",
    "        total = 0\n",
    "        on_time = 0\n",
    "\n",
    "        # Each \"service\" corresponds to a scheduled service pattern.\n",
    "        # Metrics provides counts within tolerance / outside tolerance.\n",
    "        for svc in data.get(\"Services\", []):\n",
    "            metrics = svc.get(\"Metrics\", [])\n",
    "            if not metrics:\n",
    "                continue\n",
    "            m = metrics[0]\n",
    "            total += int(m[\"num_not_tolerance\"]) + int(m[\"num_tolerance\"])\n",
    "            on_time += int(m[\"num_tolerance\"])\n",
    "\n",
    "        records.append({\n",
    "            \"date\": date_str,\n",
    "            \"departure_rail_station_crs\": from_loc,\n",
    "            \"destination_rail_station_crs\": to_loc,\n",
    "            \"service_count_total\": total,\n",
    "            \"service_count_ontime\": on_time,\n",
    "        })\n",
    "\n",
    "        time.sleep(1)  # polite delay to reduce proxy errors / rate sensitivity\n",
    "        cur += timedelta(days=1)\n",
    "\n",
    "    return pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21976d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def daterange(start_date: str, end_date: str):\n",
    "    \"\"\"\n",
    "    Generates YYYY-MM-DD strings for each day in a range (inclusive).\n",
    "    Used to detect missing days from cached CSVs.\n",
    "    \"\"\"\n",
    "    cur = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    while cur <= end:\n",
    "        yield cur.strftime(\"%Y-%m-%d\")\n",
    "        cur += timedelta(days=1)\n",
    "\n",
    "\n",
    "def get_or_build_rail(route: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rail caching strategy:\n",
    "      - Try local rail CSV\n",
    "      - Else download from GCS cache if enabled\n",
    "      - Identify which days are missing\n",
    "      - Only fetch missing days (by fetching min->max missing and filtering)\n",
    "      - Upsert into cached CSV and re-upload\n",
    "\n",
    "    Rationale:\n",
    "      - HSP API is slow/flaky, so caching per-route saves huge time.\n",
    "      - This supports the \"run once, rerun to fill gaps\" approach.\n",
    "    \"\"\"\n",
    "    from_loc = route[\"from_loc\"]\n",
    "    to_loc = route[\"to_loc\"]\n",
    "\n",
    "    rail_csv = DATA_DIR / f\"rail_{from_loc}_{to_loc}_{START_DATE}_to_{END_DATE}.csv\"\n",
    "    rail_blob = rail_csv.name\n",
    "\n",
    "    # local → gcs download\n",
    "    ensure_local_from_gcs(BUCKET_NAME, rail_blob, rail_csv, SERVICE_ACCOUNT_FILE)\n",
    "    existing = pd.read_csv(rail_csv) if rail_csv.exists() else pd.DataFrame()\n",
    "\n",
    "    existing_dates = set(existing[\"date\"].astype(str)) if not existing.empty else set()\n",
    "    all_dates = set(daterange(START_DATE, END_DATE))\n",
    "    missing = sorted(all_dates - existing_dates)\n",
    "\n",
    "    print(f\"[INFO] Rail cache {from_loc}->{to_loc}: cached={len(existing_dates)} missing={len(missing)}\")\n",
    "\n",
    "    if missing:\n",
    "        # Fetch only the range covering missing days, then filter down.\n",
    "        # This keeps code simple while still avoiding refetching already cached days.\n",
    "        fetched = get_daily_service_metrics(from_loc, to_loc, min(missing), max(missing))\n",
    "        fetched = fetched[fetched[\"date\"].isin(missing)].reset_index(drop=True)\n",
    "\n",
    "        if existing.empty:\n",
    "            out = fetched\n",
    "        else:\n",
    "            out = pd.concat([existing, fetched], ignore_index=True)\n",
    "            out = out.drop_duplicates(\n",
    "                subset=[\"date\", \"departure_rail_station_crs\", \"destination_rail_station_crs\"],\n",
    "                keep=\"last\"\n",
    "            )\n",
    "            out = out.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "        out.to_csv(rail_csv, index=False)\n",
    "        print(f\"[INFO] Saved rail -> {rail_csv}\")\n",
    "        upload_file_to_gcs(rail_csv, BUCKET_NAME, rail_blob, SERVICE_ACCOUNT_FILE)\n",
    "        return out\n",
    "\n",
    "    if existing.empty:\n",
    "        print(\"[WARNING] Rail cache empty and nothing fetched (unexpected).\")\n",
    "    return existing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c038a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded rainfall cache -> data\\rainfall_00534_bramham_2022-05-01_to_2022-05-30.csv\n",
      "[INFO] Rail cache LDS->KGX: cached=30 missing=0\n",
      "[INFO] Saved final -> data\\final_LDS_KGX_00534_bramham_2022-05-01_to_2022-05-30.csv\n",
      "[INFO] Uploaded -> gs://de-candidate-task-results-sp/final_LDS_KGX_00534_bramham_2022-05-01_to_2022-05-30.csv\n",
      "[INFO] Loaded rainfall cache -> data\\rainfall_00516_bradford_2022-05-01_to_2022-05-30.csv\n",
      "[INFO] Rail cache LDS->BDQ: cached=30 missing=0\n",
      "[INFO] Saved final -> data\\final_LDS_BDQ_00516_bradford_2022-05-01_to_2022-05-30.csv\n",
      "[INFO] Uploaded -> gs://de-candidate-task-results-sp/final_LDS_BDQ_00516_bradford_2022-05-01_to_2022-05-30.csv\n",
      "[INFO] Saved combined final -> data\\final_ALL_ROUTES_2022-05-01_to_2022-05-30.csv\n",
      "[INFO] Uploaded -> gs://de-candidate-task-results-sp/final_ALL_ROUTES_2022-05-01_to_2022-05-30.csv\n"
     ]
    }
   ],
   "source": [
    "def build_final_dataset(rail_df: pd.DataFrame, rainfall_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Final join step: align rail date to rainfall \"night before\" by date.\n",
    "\n",
    "    - Left join: keep rail rows even if rainfall missing (then rainfall becomes 0.0).\n",
    "    - Coerce types and enforce final schema / column order.\n",
    "    \"\"\"\n",
    "    out = rail_df.merge(rainfall_df, on=\"date\", how=\"left\")\n",
    "\n",
    "    # Schema enforcement: ensures consistent CSV output + BigQuery load stability.\n",
    "    out[\"rainfall_nightbefore_mm\"] = pd.to_numeric(out[\"rainfall_nightbefore_mm\"], errors=\"coerce\").fillna(0.0)\n",
    "    out[\"service_count_total\"] = pd.to_numeric(out[\"service_count_total\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    out[\"service_count_ontime\"] = pd.to_numeric(out[\"service_count_ontime\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    # Exact schema ordering required by the task\n",
    "    out = out[\n",
    "        [\n",
    "            \"date\",\n",
    "            \"departure_rail_station_crs\",\n",
    "            \"destination_rail_station_crs\",\n",
    "            \"weather_station\",\n",
    "            \"rainfall_nightbefore_mm\",\n",
    "            \"service_count_total\",\n",
    "            \"service_count_ontime\",\n",
    "        ]\n",
    "    ]\n",
    "    return out\n",
    "\n",
    "final_frames = []\n",
    "\n",
    "for route in ROUTES:\n",
    "    # Step 1: rainfall (cached)\n",
    "    rain_df = get_or_build_rainfall(route)\n",
    "\n",
    "    # Step 2: rail metrics (cached + fill gaps)\n",
    "    rail_df = get_or_build_rail(route)\n",
    "\n",
    "    # Step 3: merge into required schema\n",
    "    final_df = build_final_dataset(rail_df, rain_df)\n",
    "\n",
    "    # Save per-route final output (useful for debugging + partial reruns)\n",
    "    from_loc = route[\"from_loc\"]\n",
    "    to_loc = route[\"to_loc\"]\n",
    "    station_id = route[\"weather_station_id\"]\n",
    "    station_name = route[\"weather_station_name\"]\n",
    "\n",
    "    final_csv = DATA_DIR / f\"final_{from_loc}_{to_loc}_{station_id}_{station_name}_{START_DATE}_to_{END_DATE}.csv\"\n",
    "    final_df.to_csv(final_csv, index=False)\n",
    "    print(f\"[INFO] Saved final -> {final_csv}\")\n",
    "\n",
    "    upload_file_to_gcs(final_csv, BUCKET_NAME, final_csv.name, SERVICE_ACCOUNT_FILE)\n",
    "\n",
    "    final_frames.append(final_df)\n",
    "\n",
    "# Combined output across routes (single file is convenient for BigQuery ingestion)\n",
    "combined_final = (\n",
    "    pd.concat(final_frames, ignore_index=True)\n",
    "      .sort_values([\"departure_rail_station_crs\", \"destination_rail_station_crs\", \"date\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "combined_csv = DATA_DIR / f\"final_ALL_ROUTES_{START_DATE}_to_{END_DATE}.csv\"\n",
    "combined_final.to_csv(combined_csv, index=False)\n",
    "print(f\"[INFO] Saved combined final -> {combined_csv}\")\n",
    "\n",
    "upload_file_to_gcs(combined_csv, BUCKET_NAME, combined_csv.name, SERVICE_ACCOUNT_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30c43ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Still missing rail dates: 0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "all_dates = set(daterange(START_DATE, END_DATE))\n",
    "have_dates = set(rail_df[\"date\"].astype(str)) if not rail_df.empty else set()\n",
    "still_missing = sorted(all_dates - have_dates)\n",
    "\n",
    "print(\"[INFO] Still missing rail dates:\", len(still_missing))\n",
    "print(still_missing[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ffca067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded staging: searchlab-bq-training.de_candidate_task_seanparrott.daily_route_metrics_staging\n",
      "[INFO] Staging rows: 60\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "SERVICE_ACCOUNT_FILE = \"searchlab-bq-training-sp-key.json\"\n",
    "creds = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE)\n",
    "\n",
    "PROJECT_ID = \"searchlab-bq-training\"\n",
    "DATASET_ID = \"de_candidate_task_seanparrott\"\n",
    "\n",
    "BUCKET_NAME = \"de-candidate-task-results-sp\"\n",
    "GCS_URI = f\"gs://{BUCKET_NAME}/final_ALL_ROUTES_2022-05-01_to_2022-05-30.csv\"\n",
    "\n",
    "TARGET_TABLE  = f\"{PROJECT_ID}.{DATASET_ID}.daily_route_metrics\"\n",
    "STAGING_TABLE = f\"{PROJECT_ID}.{DATASET_ID}.daily_route_metrics_staging\"\n",
    "\n",
    "client = bigquery.Client(credentials=creds, project=PROJECT_ID)\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    schema=[\n",
    "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
    "        bigquery.SchemaField(\"departure_rail_station_crs\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"destination_rail_station_crs\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"weather_station\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"rainfall_nightbefore_mm\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"service_count_total\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"service_count_ontime\", \"INTEGER\"),\n",
    "    ],\n",
    "    skip_leading_rows=1,\n",
    "    source_format=bigquery.SourceFormat.CSV,\n",
    "    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,  # staging is always replaced\n",
    ")\n",
    "\n",
    "job_config.time_partitioning = bigquery.TimePartitioning(\n",
    "    type_=bigquery.TimePartitioningType.DAY,\n",
    "    field=\"date\",\n",
    ")\n",
    "\n",
    "job = client.load_table_from_uri(GCS_URI, STAGING_TABLE, job_config=job_config)\n",
    "job.result()\n",
    "\n",
    "print(\"[INFO] Loaded staging:\", STAGING_TABLE)\n",
    "print(\"[INFO] Staging rows:\", client.get_table(STAGING_TABLE).num_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50af97eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Target table exists: searchlab-bq-training.de_candidate_task_seanparrott.daily_route_metrics\n",
      "------ BigQuery Upsert Summary ------\n",
      "Rows before merge: 60\n",
      "Rows inserted/updated: 60\n",
      "Rows after merge: 60\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from google.api_core.exceptions import NotFound\n",
    "\n",
    "# 1) Ensure target table exists (create empty table if it doesn't)\n",
    "try:\n",
    "    client.get_table(TARGET_TABLE)\n",
    "    print(\"[INFO] Target table exists:\", TARGET_TABLE)\n",
    "except NotFound:\n",
    "    schema = client.get_table(STAGING_TABLE).schema\n",
    "    table = bigquery.Table(TARGET_TABLE, schema=schema)\n",
    "    table.time_partitioning = bigquery.TimePartitioning(type_=bigquery.TimePartitioningType.DAY, field=\"date\")\n",
    "    client.create_table(table)\n",
    "    print(\"[INFO] Created target table:\", TARGET_TABLE)\n",
    "\n",
    "# 2) MERGE staging -> target (upsert)\n",
    "merge_sql = f\"\"\"\n",
    "MERGE `{TARGET_TABLE}` T\n",
    "USING `{STAGING_TABLE}` S\n",
    "ON  T.date = S.date\n",
    "AND T.departure_rail_station_crs = S.departure_rail_station_crs\n",
    "AND T.destination_rail_station_crs = S.destination_rail_station_crs\n",
    "AND T.weather_station = S.weather_station\n",
    "\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "  rainfall_nightbefore_mm = S.rainfall_nightbefore_mm,\n",
    "  service_count_total = S.service_count_total,\n",
    "  service_count_ontime = S.service_count_ontime\n",
    "\n",
    "WHEN NOT MATCHED THEN INSERT (\n",
    "  date,\n",
    "  departure_rail_station_crs,\n",
    "  destination_rail_station_crs,\n",
    "  weather_station,\n",
    "  rainfall_nightbefore_mm,\n",
    "  service_count_total,\n",
    "  service_count_ontime\n",
    ") VALUES (\n",
    "  S.date,\n",
    "  S.departure_rail_station_crs,\n",
    "  S.destination_rail_station_crs,\n",
    "  S.weather_station,\n",
    "  S.rainfall_nightbefore_mm,\n",
    "  S.service_count_total,\n",
    "  S.service_count_ontime\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "before_count = client.get_table(TARGET_TABLE).num_rows\n",
    "\n",
    "merge_job = client.query(merge_sql)\n",
    "merge_job.result()\n",
    "\n",
    "after_count = client.get_table(TARGET_TABLE).num_rows\n",
    "affected = merge_job.num_dml_affected_rows\n",
    "\n",
    "print(\"------ BigQuery Upsert Summary ------\")\n",
    "print(\"Rows before merge:\", before_count)\n",
    "print(\"Rows inserted/updated:\", affected)\n",
    "print(\"Rows after merge:\", after_count)\n",
    "print(\"--------------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5db29a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] View ready: searchlab-bq-training.de_candidate_task_seanparrott.v_daily_route_metrics\n"
     ]
    }
   ],
   "source": [
    "VIEW_REF = f\"{PROJECT_ID}.{DATASET_ID}.v_daily_route_metrics\"\n",
    "\n",
    "view_sql = f\"\"\"\n",
    "CREATE OR REPLACE VIEW `{VIEW_REF}` AS\n",
    "SELECT\n",
    "  date,\n",
    "  departure_rail_station_crs,\n",
    "  destination_rail_station_crs,\n",
    "  weather_station,\n",
    "  rainfall_nightbefore_mm,\n",
    "  service_count_total,\n",
    "  service_count_ontime\n",
    "FROM `{TARGET_TABLE}`;\n",
    "\"\"\"\n",
    "\n",
    "client.query(view_sql).result()\n",
    "print(\"[INFO] View ready:\", VIEW_REF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33e6e63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\epoch_bpjmdqk\\Documents\\Techincal Task\\venv\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>departure_rail_station_crs</th>\n",
       "      <th>destination_rail_station_crs</th>\n",
       "      <th>weather_station</th>\n",
       "      <th>rainfall_nightbefore_mm</th>\n",
       "      <th>service_count_total</th>\n",
       "      <th>service_count_ontime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>LDS</td>\n",
       "      <td>BDQ</td>\n",
       "      <td>bradford</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>LDS</td>\n",
       "      <td>KGX</td>\n",
       "      <td>bramham</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-05-02</td>\n",
       "      <td>LDS</td>\n",
       "      <td>BDQ</td>\n",
       "      <td>bradford</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-05-02</td>\n",
       "      <td>LDS</td>\n",
       "      <td>KGX</td>\n",
       "      <td>bramham</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-05-03</td>\n",
       "      <td>LDS</td>\n",
       "      <td>BDQ</td>\n",
       "      <td>bradford</td>\n",
       "      <td>1.4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date departure_rail_station_crs destination_rail_station_crs  \\\n",
       "0  2022-05-01                        LDS                          BDQ   \n",
       "1  2022-05-01                        LDS                          KGX   \n",
       "2  2022-05-02                        LDS                          BDQ   \n",
       "3  2022-05-02                        LDS                          KGX   \n",
       "4  2022-05-03                        LDS                          BDQ   \n",
       "\n",
       "  weather_station  rainfall_nightbefore_mm  service_count_total  \\\n",
       "0        bradford                      2.2                    0   \n",
       "1         bramham                      0.0                    0   \n",
       "2        bradford                      0.0                    6   \n",
       "3         bramham                      0.0                    9   \n",
       "4        bradford                      1.4                    6   \n",
       "\n",
       "   service_count_ontime  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     6  \n",
       "3                     7  \n",
       "4                     5  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM `{VIEW_REF}`\n",
    "ORDER BY date, departure_rail_station_crs, destination_rail_station_crs\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "client.query(query).to_dataframe().head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
